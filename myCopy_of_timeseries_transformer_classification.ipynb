{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Timeseries classification with a Transformer model\n",
        "\n",
        "**Author:** [Theodoros Ntakouris](https://github.com/ntakouris)<br>\n",
        "**Date created:** 2021/06/25<br>\n",
        "**Last modified:** 2021/08/05<br>\n",
        "**Description:** This notebook demonstrates how to do timeseries classification using a Transformer model."
      ],
      "metadata": {
        "id": "Dx8I7rBIeQ3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7oRMuovfKwp",
        "outputId": "7dfe9dec-f5c4-4452-ea52-29253101e346"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This is the Transformer architecture from\n",
        "[Attention Is All You Need](https://arxiv.org/abs/1706.03762),\n",
        "applied to timeseries instead of natural language.\n",
        "\n",
        "This example requires TensorFlow 2.4 or higher.\n",
        "\n",
        "## Load the dataset\n",
        "\n",
        "We are going to use the same dataset and preprocessing as the\n",
        "[TimeSeries Classification from Scratch](https://keras.io/examples/timeseries/timeseries_classification_from_scratch)\n",
        "example."
      ],
      "metadata": {
        "id": "tdpMxwYleQ4L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# def readucr(filename):\n",
        "#     data = np.loadtxt(filename, delimiter=\"\\t\")\n",
        "#     y = data[:, 0]\n",
        "#     x = data[:, 1:]\n",
        "#     return x, y.astype(int)\n",
        "\n",
        "\n",
        "# root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
        "\n",
        "# x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
        "# x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n",
        "\n",
        "# x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "# x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
        "def load_data(data_path):\n",
        "    \"\"\"\n",
        "    Loading of the dataset provided\n",
        "    Edit the code below\n",
        "    \"\"\"\n",
        "    data=pd.read_pickle(data_path)\n",
        "    return data\n",
        "\n",
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    A standard nan removal to be added.\n",
        "    Add more preprocessing steps if needed.\n",
        "    \"\"\"\n",
        "    return data\n",
        "\n",
        "def split_train_test(data):\n",
        "    \"\"\"\n",
        "    Splitting the data into train, test, validation \n",
        "    \"\"\"\n",
        "    train, test = train_test_split(data,test_size=0.4, random_state=42,stratify=data['labels'])\n",
        "    test, val = train_test_split(test,test_size=0.5, random_state=42, stratify=test['labels'])\n",
        "\n",
        "    return train, test, val\n",
        "\n",
        "data = load_data(\"/content/drive/MyDrive/Colab Notebooks/data.pkl\")\n",
        "data = preprocess_data(data)\n",
        "\n",
        "train, test, val = split_train_test(data)\n",
        "\n",
        "x_train = np.array(train.dim_0.values.tolist())\n",
        "y_train = np.array(train.labels).astype(int)\n",
        "\n",
        "X_test = np.array(test.dim_0.values.tolist())\n",
        "y_test = np.array(train.labels).astype(int)\n",
        "\n",
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "n_classes = 2\n",
        "\n",
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]\n",
        "\n",
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0"
      ],
      "outputs": [],
      "metadata": {
        "id": "PChKxwBkeQ4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "Our model processes a tensor of shape `(batch size, sequence length, features)`,\n",
        "where `sequence length` is the number of time steps and `features` is each input\n",
        "timeseries.\n",
        "\n",
        "You can replace your classification RNN layers with this one: the\n",
        "inputs are fully compatible!"
      ],
      "metadata": {
        "id": "v_b1If-0eQ43"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "outputs": [],
      "metadata": {
        "id": "nOfXovWXeQ5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We include residual connections, layer normalization, and dropout.\n",
        "The resulting layer can be stacked multiple times.\n",
        "\n",
        "The projection layers are implemented through `keras.layers.Conv1D`."
      ],
      "metadata": {
        "id": "CcyL4cGmeQ5c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "source": [
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "hXZmBUhSeQ57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main part of our model is now complete. We can stack multiple of those\n",
        "`transformer_encoder` blocks and we can also proceed to add the final\n",
        "Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n",
        "layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n",
        "our model down to a vector of features for each data point in the current\n",
        "batch. A common way to achieve this is to use a pooling layer. For\n",
        "this example, a `GlobalAveragePooling1D` layer is sufficient."
      ],
      "metadata": {
        "id": "hEHJ98RReQ6F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "source": [
        "\n",
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    return keras.Model(inputs, outputs)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "kwCauTzPeQ6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate"
      ],
      "metadata": {
        "id": "kKcw6I5IeQ6k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics=keras.metrics.BinaryAccuracy(),\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=200,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "model.evaluate(x_test, y_test, verbose=1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_12 (InputLayer)          [(None, 500, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_88 (LayerN  (None, 500, 1)      2           ['input_12[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_44 (Multi  (None, 500, 1)      7169        ['layer_normalization_88[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_88[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_99 (Dropout)           (None, 500, 1)       0           ['multi_head_attention_44[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_88 (TFOpL  (None, 500, 1)      0           ['dropout_99[0][0]',             \n",
            " ambda)                                                           'input_12[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_89 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_88[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_88 (Conv1D)             (None, 500, 4)       8           ['layer_normalization_89[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_100 (Dropout)          (None, 500, 4)       0           ['conv1d_88[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_89 (Conv1D)             (None, 500, 1)       5           ['dropout_100[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_89 (TFOpL  (None, 500, 1)      0           ['conv1d_89[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_88[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_90 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_89[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_45 (Multi  (None, 500, 1)      7169        ['layer_normalization_90[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_90[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_101 (Dropout)          (None, 500, 1)       0           ['multi_head_attention_45[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_90 (TFOpL  (None, 500, 1)      0           ['dropout_101[0][0]',            \n",
            " ambda)                                                           'tf.__operators__.add_89[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_91 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_90[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_90 (Conv1D)             (None, 500, 4)       8           ['layer_normalization_91[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_102 (Dropout)          (None, 500, 4)       0           ['conv1d_90[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_91 (Conv1D)             (None, 500, 1)       5           ['dropout_102[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_91 (TFOpL  (None, 500, 1)      0           ['conv1d_91[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_90[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_92 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_91[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_46 (Multi  (None, 500, 1)      7169        ['layer_normalization_92[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_92[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_103 (Dropout)          (None, 500, 1)       0           ['multi_head_attention_46[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_92 (TFOpL  (None, 500, 1)      0           ['dropout_103[0][0]',            \n",
            " ambda)                                                           'tf.__operators__.add_91[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_93 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_92[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_92 (Conv1D)             (None, 500, 4)       8           ['layer_normalization_93[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_104 (Dropout)          (None, 500, 4)       0           ['conv1d_92[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_93 (Conv1D)             (None, 500, 1)       5           ['dropout_104[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_93 (TFOpL  (None, 500, 1)      0           ['conv1d_93[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_92[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_94 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_93[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_47 (Multi  (None, 500, 1)      7169        ['layer_normalization_94[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_94[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_105 (Dropout)          (None, 500, 1)       0           ['multi_head_attention_47[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_94 (TFOpL  (None, 500, 1)      0           ['dropout_105[0][0]',            \n",
            " ambda)                                                           'tf.__operators__.add_93[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_95 (LayerN  (None, 500, 1)      2           ['tf.__operators__.add_94[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_94 (Conv1D)             (None, 500, 4)       8           ['layer_normalization_95[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_106 (Dropout)          (None, 500, 4)       0           ['conv1d_94[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_95 (Conv1D)             (None, 500, 1)       5           ['dropout_106[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_95 (TFOpL  (None, 500, 1)      0           ['conv1d_95[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_94[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_11 (G  (None, 500)         0           ['tf.__operators__.add_95[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 128)          64128       ['global_average_pooling1d_11[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dropout_107 (Dropout)          (None, 128)          0           ['dense_22[0][0]']               \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 1)            129         ['dropout_107[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 93,001\n",
            "Trainable params: 93,001\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "37/37 [==============================] - 24s 568ms/step - loss: 0.8789 - binary_accuracy: 0.5294 - val_loss: 0.6881 - val_binary_accuracy: 0.5685\n",
            "Epoch 2/200\n",
            "37/37 [==============================] - 22s 599ms/step - loss: 0.7884 - binary_accuracy: 0.5536 - val_loss: 0.6460 - val_binary_accuracy: 0.6176\n",
            "Epoch 3/200\n",
            "37/37 [==============================] - 21s 570ms/step - loss: 0.7226 - binary_accuracy: 0.5896 - val_loss: 0.6183 - val_binary_accuracy: 0.6565\n",
            "Epoch 4/200\n",
            "37/37 [==============================] - 20s 551ms/step - loss: 0.6971 - binary_accuracy: 0.6192 - val_loss: 0.5970 - val_binary_accuracy: 0.6768\n",
            "Epoch 5/200\n",
            "37/37 [==============================] - 21s 559ms/step - loss: 0.6566 - binary_accuracy: 0.6413 - val_loss: 0.5825 - val_binary_accuracy: 0.6819\n",
            "Epoch 6/200\n",
            "37/37 [==============================] - 21s 572ms/step - loss: 0.6348 - binary_accuracy: 0.6603 - val_loss: 0.5707 - val_binary_accuracy: 0.6971\n",
            "Epoch 7/200\n",
            "37/37 [==============================] - 21s 567ms/step - loss: 0.6176 - binary_accuracy: 0.6870 - val_loss: 0.5620 - val_binary_accuracy: 0.6937\n",
            "Epoch 8/200\n",
            "37/37 [==============================] - 21s 559ms/step - loss: 0.6031 - binary_accuracy: 0.6722 - val_loss: 0.5507 - val_binary_accuracy: 0.7039\n",
            "Epoch 9/200\n",
            "37/37 [==============================] - 21s 562ms/step - loss: 0.5854 - binary_accuracy: 0.6904 - val_loss: 0.5448 - val_binary_accuracy: 0.7073\n",
            "Epoch 10/200\n",
            "37/37 [==============================] - 21s 567ms/step - loss: 0.5756 - binary_accuracy: 0.6972 - val_loss: 0.5389 - val_binary_accuracy: 0.7191\n",
            "Epoch 11/200\n",
            "37/37 [==============================] - ETA: 0s - loss: 0.5530 - binary_accuracy: 0.7162"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-87fa9a06dc22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1429\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1432\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {
        "id": "yFIB-y26eQ61",
        "outputId": "560c8e58-59ee-4a00-d41e-a103319d26b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "In about 110-120 epochs (25s each on Colab), the model reaches a training\n",
        "accuracy of ~0.95, validation accuracy of ~84 and a testing\n",
        "accuracy of ~85, without hyperparameter tuning. And that is for a model\n",
        "with less than 100k parameters. Of course, parameter count and accuracy could be\n",
        "improved by a hyperparameter search and a more sophisticated learning rate\n",
        "schedule, or a different optimizer.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/timeseries_transformer_classification) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/timeseries_transformer_classification)."
      ],
      "metadata": {
        "id": "s9OpzqgReQ7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v8TUf_VwAn5z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "myCopy of timeseries_transformer_classification",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}